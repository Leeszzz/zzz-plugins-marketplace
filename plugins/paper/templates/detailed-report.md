# Detailed 模式输出模板

此模板定义了详细模式（学术分析）的单篇文献报告格式。

---

## 文档结构

```markdown
# 文献详细分析：{title}

> 原始文件: {filename}
> 分析时间: {timestamp}
> 分析模式: 详细

---

## 📋 元信息

| 字段 | 内容 |
|------|------|
| 标题 | {title} |
| 作者 | {authors} |
| 年份 | {year} |
| 期刊/会议 | {venue} |
| DOI/链接 | {doi_or_link} |
| 关键词 | {keywords} |

---

## 1. 研究背景与动机

### 1.1 研究问题

{research_question}

### 1.2 现有工作的不足

{existing_limitations}

---

## 2. 研究方法

### 2.1 整体框架

{overall_framework}

### 2.2 关键技术

{key_techniques}

### 2.3 实验设置

{experimental_setup}

---

## 3. 主要结果

### 3.1 定量结果

{quantitative_results}

### 3.2 定性分析

{qualitative_analysis}

---

## 4. 讨论与结论

### 4.1 核心贡献

{core_contributions}

### 4.2 实际意义

{practical_implications}

---

## 5. 局限性与未来工作

### 5.1 当前局限

{limitations}

### 5.2 未来方向

{future_work}

---

## 📝 个人笔记

<!-- 在此添加你的想法和笔记 -->

---

## 📚 相关文献

{references_if_available}

```

---

## 字段说明

| 字段 | 说明 | 必填 |
|------|------|------|
| `title` | 论文完整标题 | ✅ |
| `filename` | 原始文件名 | ✅ |
| `authors` | 完整作者列表 | ✅ |
| `year` | 发表年份 | ✅ |
| `venue` | 期刊或会议名称 | 可选 |
| `doi_or_link` | DOI 或论文链接 | 可选 |
| `keywords` | 关键词列表 | 可选 |
| 各章节内容 | 分析内容 | ✅ |

---

## 文件命名规则

详细模式为每篇文献生成单独文件：

```
输入文件: attention-is-all-you-need.pdf
输出文件: attention-is-all-you-need-summary.md

输入文件: bert_paper.md
输出文件: bert_paper-summary.md
```

---

## 内容要求

### 研究背景（约 200-300 字）
- 简述研究领域的背景
- 明确指出要解决的问题
- 总结现有方法的不足

### 研究方法（约 300-500 字）
- 描述整体方法框架
- 解释关键技术创新点
- 说明实验设置（数据集、指标、基线）

### 主要结果（约 200-400 字）
- 列出关键定量指标
- 提供定性分析或案例

### 讨论与结论（约 200-300 字）
- 总结核心贡献（3-5 点）
- 讨论实际应用价值

### 局限性与未来（约 100-200 字）
- 指出论文承认的局限
- 列举可能的后续方向

---

## 示例输出

```markdown
# 文献详细分析：Attention Is All You Need

> 原始文件: attention.pdf
> 分析时间: 2024-01-07 14:30:00
> 分析模式: 详细

---

## 📋 元信息

| 字段 | 内容 |
|------|------|
| 标题 | Attention Is All You Need |
| 作者 | Ashish Vaswani, Noam Shazeer, Niki Parmar, et al. |
| 年份 | 2017 |
| 期刊/会议 | NeurIPS 2017 |
| DOI/链接 | arXiv:1706.03762 |
| 关键词 | Transformer, Attention, Sequence-to-Sequence |

---

## 1. 研究背景与动机

### 1.1 研究问题

序列到序列建模（如机器翻译）传统上依赖 RNN 或 CNN，这些方法在处理长距离依赖时存在困难，且难以并行化训练。本文探索能否完全依赖注意力机制来建模序列。

### 1.2 现有工作的不足

- RNN 的顺序性限制了并行训练
- CNN 需要多层才能捕捉远距离依赖
- 现有注意力机制仅作为 RNN 的补充，而非替代

---

## 2. 研究方法

### 2.1 整体框架

Transformer 采用编码器-解码器架构，每个编码器层包含多头自注意力和前馈网络，解码器额外增加编码器-解码器注意力层。

### 2.2 关键技术

1. **多头注意力**：将注意力分成多个头并行计算，捕捉不同子空间的特征
2. **位置编码**：使用正弦函数编码位置信息，弥补注意力机制缺乏位置感知
3. **残差连接与层归一化**：稳定深层网络训练

### 2.3 实验设置

- 数据集：WMT 2014 英德/英法翻译
- 指标：BLEU 分数
- 基线：ConvS2S、GNMT 等

---

## 3. 主要结果

### 3.1 定量结果

| 任务 | 模型 | BLEU |
|------|------|------|
| 英德 | Transformer (big) | 28.4 |
| 英法 | Transformer (big) | 41.0 |

超越所有现有模型，且训练时间大幅减少。

### 3.2 定性分析

注意力可视化显示模型能够学习到语法结构和长距离依赖关系。

---

## 4. 讨论与结论

### 4.1 核心贡献

1. 提出完全基于注意力的 Transformer 架构
2. 实现高度并行化训练
3. 在翻译任务上达到 SOTA
4. 为后续 BERT、GPT 等模型奠定基础

### 4.2 实际意义

Transformer 已成为 NLP 领域的基础架构，广泛应用于机器翻译、文本生成、问答等任务。

---

## 5. 局限性与未来工作

### 5.1 当前局限

- 自注意力复杂度为 O(n²)，对超长序列不友好
- 缺乏归纳偏置，需要大量数据训练

### 5.2 未来方向

- 稀疏注意力机制
- 预训练语言模型
- 多模态 Transformer

---

## 📝 个人笔记

<!-- 在此添加你的想法和笔记 -->

```
